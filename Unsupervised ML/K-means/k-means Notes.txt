Source :
https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/

Q:1	What is Clustering?
-->	Clustering is the process of dividing the entire data into groups (also known as clusters) based on the patterns in the data.

Q:2	How is Clustering an Unsupervised Learning Problem?
-->	So,when we have a target variable to predict based on a given set of predictors or independent variables, such problems are called supervised learning problems.

Such problems, without any fixed target variable, are known as unsupervised learning problems. In these problems, we only have the
independent variables and no target/dependent variable.

In clustering, we do not have a target to predict. We look at the data and then try to club similar
observations and form different groups. Hence it is an unsupervised learning problem.

Q:3	Properties of Clusters:
-->
Property 1:	All the data points in a cluster should be similar to each other.

Property 2:	The data points from different clusters should be as different as possible.

Q:4	Applications of Clustering in Real-World Scenarios?
-->
1. Customer Segmentation
2. Document Clustering:
	This is another common application of clustering. Let’s say you have multiple documents and you need to
	cluster similar documents together. Clustering helps us group these documents such that similar documents are in the same clusters
3. Image Segmentation
	We can also use clustering to perform image segmentation. Here, we try to club similar pixels in the image
	together. We can apply clustering to create clusters having similar pixels in the same group.
4. Recommendation Engines
	Clustering can also be used in recommendation engines. Let’s say you want to recommend songs to your
	friends. You can look at the songs liked by that person and then use clustering to find similar songs and
	finally recommend the most similar songs.

Q:5	What are the Different Evaluation Metrics for Clustering
--> The primary aim of clustering is not just to make clusters, but to make good and meaningful one cluster.To evaluate the quality of our 	clusters we will use Evaluation Metrics:

1. Inertia : It tells us how far the points within a cluster are. So, iner tia actually calculates the sum of distances of all the points within 	a cluster from the centroid of that cluster. We calculate this for all the clusters and the final inertial value is the sum of all these 	distances. This distance within the clusters is known as intracluster distance. So, inertia gives us the sum of intracluster distances.
	We want the points within the same cluster to be similar to each other, right? Hence, the distance between them should be as low as 	possible.
	**Keeping this in mind, we can say that the lesser the inertia value, the better our clusters are.

2. Dunn Index : We want to maximize the Dunn index. The more the value of the Dunn index, the better will be the clusters.
	We now know that inertia tries to minimize the intracluster distance. It is trying to make more compact clusters.
	Let me put it this way – if the distance between the centroid of a cluster and the points in that cluster is
	small, it means that the points are closer to each other. So, inertia makes sure that the first property of
	clusters is satisfied. But it does not care about the second property – that different clusters should be as
	different from each other as possible. This is where Dunn index can come into action.
	Along with the distance between the centroid and points, the Dunn index also takes into account the
	distance between two clusters. This distance between the centroids of two different clusters is known as inter-cluster distance.

	** Dunn index is the ratio of the minimum of inter-cluster distances and maximum of intracluster distances.
	** We want to maximize the Dunn index. The more the value of the Dunn index, the better will be the clusters.

________________________________________________________________________________________________________________________________________________
________________________________________________________________________________________________________________________________________________

			#######   K-Means Clustering     ######


Q:1	What is Meant by the K-Means Clustering Algorithm?
-->	K-Means clustering is an unsupervised learning algorithm. There is no labeled data for this clustering, unlike in supervised learning. 	K-Means performs the division of objects into clusters that share similarities and are dissimilar to the objects belonging to another 	cluster.
	K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to
	assign a point to a cluster. In K-Means, each cluster is associated with a centroid.
	
	**The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respectivecluster 	centroid.


Q :	How to Choose the Value of "K number of clusters" in K-Means Clustering?
	Although there are many choices available for choosing the optimal number of clusters, the Elbow Method is one of the most popular and 	appropriate methods. The Elbow Method uses the idea of WCSS value, which is short for for Within Cluster Sum of Squares. WCSS defines 	the total number of variations within a cluster. 
	WCSS is defined as the sum of the squared distance between each member of the cluster and its centroid.


$$$$$$$$   Working priciple     $$$$$$$$$

major steps of K-means : Data --> Similarity  --> Distance --> Euclidean Distance

Step 1 : Value of k (i.e, centroid) will be randomly initalized. let's say 2

step 2 : as of now we are going to build two cluster lte's say c1 and c2. We will check the distance between c1 and point3 And similarly between 	c2 and point3. We will calculate the Euclidean distance And which cluster will having the minimum distance that point will be considered 	to that particular cluster.
	@@ let's say point 3 will be blonging to centroid c2 bcoz distance is minimum to centroid2/cluster2.

step 3: Now, Update the cluster/centroid .
	Note : Whenever we are adding any points inside the cluster, so we are going to update cluster/centroid by doing avg.

step 4 : repeat step 2 and step 3 until Maximum number of iterations are reached.


Main Steps : 1. centroid 2. Distance[compare, minm] 3. include point in that cluster and update centroid. ##Iterate it till all the data points.


Q : 	How can we decide what will be best value of K?
--> 	By using Elbow Menthod. In elbow method we draw a graph between WCSS and V . the shape of graph looks like elbow i.e, why it is named as 	elbow method. In this graph there will be a point where sudden change occur. the point where the value of wcss is minimun will be 	considerd as the value of k.
	$  We draw a curve between WSS and the number of clusters. 

NOTE : In a dataset there can be minimum  k=1 means one cluster And the maximum value of k can be total no. cloumn /features.

Q : 	Performance metrics for K-means clustering?
-->	To evaluate the quality of our clusters we will use performance Metrics/Evaluation Metrics:

	1. Dunn Index = ratio of the inter-cluster distances and intracluster distances.
	2. Silhoute Score = (bi-ai)/max(bi-ai) , 
		where   ai = mean distance within the cluster (intra cluster),
			bi = inter cluster distance.
	*** Silhoute score ranges between [-1,+1]
Q : 	Stopping Criteria for K-Means Clustering
-->	There are essentially three stopping criteria that can be adopted to stop the K-means algorithm:
	1. Centroids of newly formed clusters do not change
	2. Points remain in the same cluster
	3. Maximum number of iterations are reached

Q : 	Challenges with the K-Means Clustering Algorithm.
-->	1.One of the common challenges we face while working with K-Means is that the size of clusters is different.
	2.Another challenge with k-means is when the densities of the original points are different.

	how we randomly initialize the centroids in k-means clustering? Well, this is also potentially
	problematic because we might get different clusters every time. So, to solve this problem of random
	initialization, there is an algorithm called K-Means++ that can be used to choose the initial values, or the
	initial cluster centroids, for K-Means. 
________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

		########	K-Means++ algorithm   	########

In some cases, if the initialization of clusters is not appropriate, K-Means can result in arbitrarily bad
clusters. This is where K-Means++ helps. It specifies a procedure to initialize the cluster centers before
moving forward with the standard k-means clustering algorithm.
Using the K-Means++ algorithm, we optimize the step where we randomly pick the cluster centroid. We are
more likely to find a solution that is competitive to the optimal K-Means solution while using the KMeans++
initialization.

The steps to initialize the centroids using K-Means++ are:
step 1: The first cluster is chosen uniformly at random from the data points that we want to cluster. This is
	similar to what we do in K-Means, but instead of randomly picking all the centroids, we just pick one centroid here
step 2: Next, we compute the distance (D(x)) of each data point (x) from the cluster center that has already
	been chosen
step 3: Then next centroid will be the one whose squared distance (D(x)2) is the farthest from the current centroid.
	Then, choose the new cluster center from the data points with the probability of x being proportional to (D(x))2
step 4: We then repeat steps 2 and 3 until k clusters have been chosen


@@@ Concept of K-means ++ (own word/lamen word)
K-mean : it is all about centroid initialization, and we are randomly initializing centroid. So if we are picking centroid randomly at the end  there might be chances we won't found good cluster. This is not a good way in many cases. So we will use k-means ++

K-Means ++ : We will initally take one cluster and then we will try try to find distance from centroid to each point. And wherever we are having farthest point(as far as possible) will choose from the dataset and that point we will make another centroid . We will repeat this untill k-cluster has been choosen.
